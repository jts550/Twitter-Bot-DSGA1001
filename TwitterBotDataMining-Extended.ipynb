{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from collections import defaultdict\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import locale\n",
    "locale.setlocale( locale.LC_ALL, '' )\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLearnCurve(grid_summary, param):\n",
    "    if len(param) == 1:\n",
    "        fig, axes = plt.subplots(figsize=(8,6))\n",
    "        x_val = grid_summary[param]\n",
    "        max_score_idx = grid_summary.mean_validation_score.argmax()\n",
    "        \n",
    "        axes.plot(x_val, grid_summary['mean_validation_score'], 'C0', label='Mean AUC')\n",
    "        \n",
    "        lower = grid_summary['mean_validation_score'] - grid_summary['std_err']\n",
    "        axes.plot(x_val, lower, 'C0', label='-1 Std.Err', linestyle='--')\n",
    "        \n",
    "        upper = grid_summary['mean_validation_score'] + grid_summary['std_err']\n",
    "        axes.plot(x_val, upper, 'C0', label='+1 Std.Err', linestyle='--')\n",
    "        \n",
    "        best_lower = grid_summary.mean_validation_score.max() - grid_summary.std_err[max_score_idx]\n",
    "        xmin = x_val.min()\n",
    "        xmax = x_val.max()\n",
    "        plt.hlines(xmin=xmin, xmax=xmax, y=best_lower, color='r')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.title(\"Learning Curve: \" + str(param))\n",
    "        \n",
    "    else:\n",
    "        fig, axes = plt.subplots(figsize=(8,6))\n",
    "        plt_data = pd.pivot_table(grid_summary, index=param[0], columns=param[1])\n",
    "        col_max = plt_data['mean_validation_score'].max().idxmax()\n",
    "        row_max = plt_data['mean_validation_score'][col_max].idxmax()\n",
    "        \n",
    "        plt_data['mean_validation_score'].plot(ax=axes, figsize=(8,6))\n",
    "        \n",
    "        upper = (plt_data['mean_validation_score'] + plt_data['std_err'])\n",
    "        upper.plot(ax=axes, figsize=(8,6),alpha=0.25,linestyle='--')\n",
    "        \n",
    "        lower = (plt_data['mean_validation_score'] - plt_data['std_err'])\n",
    "        lower.plot(ax=axes, figsize=(8,6),alpha=0.25,linestyle='--')\n",
    "        \n",
    "        best_lower = plt_data['mean_validation_score'].loc[row_max, col_max] - \\\n",
    "                                plt_data['std_err'].loc[row_max, col_max]\n",
    "        xmin = plt_data.index.values.min()\n",
    "        xmax = plt_data.index.values.max()\n",
    "        plt.hlines(xmin=xmin, xmax=xmax, y=best_lower, color='r')\n",
    "        \n",
    "        plt.title(\"Learning Curve: \" + str(param))\n",
    "        \n",
    "        \n",
    "def gridSearchSummary(grid_search):\n",
    "    grid_summary = pd.DataFrame(grid_search.grid_scores_)\n",
    "    \n",
    "    params_summary = defaultdict(list)\n",
    "    for row in grid_summary.parameters:\n",
    "        for key, value in row.items():\n",
    "            params_summary[key] += [value]\n",
    "    params_summary_df = pd.DataFrame(params_summary)\n",
    "    \n",
    "    grid_summary.drop('parameters', 1, inplace=True)\n",
    "    grid_summary = params_summary_df.join(grid_summary)\n",
    "    std_err = grid_summary.cv_validation_scores.apply(lambda x: np.sqrt(np.var(x)/len(x)))\n",
    "    grid_summary.insert(grid_summary.columns.get_loc(\"mean_validation_score\")+1, 'std_err', std_err)\n",
    "    \n",
    "    return grid_summary\n",
    "\n",
    "def tuningIteration(estimator, param_grid, X, Y, pipeline=None):\n",
    "    if pipeline == None:\n",
    "        pipeline = Pipeline([('variance_thresh', VarianceThreshold()), ('estimator', estimator)])\n",
    "    grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=10, scoring='roc_auc')\n",
    "    grid_search.fit(X, Y)\n",
    "    print(\"Best Score: {:0.6}\\n\".format(grid_search.best_score_))\n",
    "    print(\"Best Params: \",grid_search.best_params_)\n",
    "    grid_summary = gridSearchSummary(grid_search)\n",
    "    plotLearnCurve(grid_summary, list(param_grid.keys()))\n",
    "\n",
    "def expectedValue(preds, truth, thresholds, fp_cost, fn_cost):\n",
    "    # fp_cost and fn_cost should be the change in revenue associated with 1000 ad requests (i.e. rCPM change)\n",
    "    # output of expected value is the expected rCPM \n",
    "    \n",
    "    class_preds = list(map(lambda x, y: 1 if x > y else 0, preds, thresholds))\n",
    "    \n",
    "    conf_mat = confusion_matrix(truth, class_preds)/len(truth)\n",
    "    cost_mat = np.array([0, fp_cost,\n",
    "                         fn_cost, 0]).reshape(2,2)\n",
    "    \n",
    "    ev = conf_mat[0,1]*cost_mat[0,1] + conf_mat[1,0]*cost_mat[1,0]\n",
    "    \n",
    "    return ev\n",
    "\n",
    "def millions(x, pos):\n",
    "    'The two args are the value and tick position'\n",
    "    return '$%1.1fM' % (x*1e-6)\n",
    "\n",
    "def to_percent(x, pos):\n",
    "    'The two args are the value and tick position'\n",
    "    return '%.0f%%' % (x*100)\n",
    "\n",
    "def plotPerformance(pipelines, names, X_train, Y_train, X_test, Y_test,\n",
    "                    fp_cost=-0.03, fn_cost=-0.06, timeframe='yearly'):\n",
    "    \n",
    "    # fp_cost based on Mopub rcpm\n",
    "    # fn_cost = 2x fp_cost - Assume buyers act broadly because of a single bad actor\n",
    "    # Twitter ad exchange request volume estimate:  \n",
    "    #   https://media.mopub.com/media/filer_public/22/b5/22b58fbf-b077-4c2c-ae41-d53d06d23dd9/mopub_global_mobile_programmatic_trends_report_-_q2_2016.pdf\n",
    "    \n",
    "    #Profit Curves\n",
    "    mil_formatter = FuncFormatter(millions)\n",
    "    pct_formatter = FuncFormatter(to_percent)\n",
    "    \n",
    "    preds = []\n",
    "    for pipeline in pipelines:\n",
    "        pipeline.fit(X_train, Y_train)\n",
    "        estimator = pipeline.named_steps['estimator']\n",
    "        if isinstance(estimator, GradientBoostingClassifier) | isinstance(estimator, SVC):\n",
    "            preds += [pipeline.decision_function(X_test)]\n",
    "        else:\n",
    "            preds += [pipeline.predict_proba(X_test)[:,1]]\n",
    "            \n",
    "    preds_zip = list(zip(preds, names))\n",
    "    \n",
    "    #Profit Curves Plot\n",
    "    fig = plt.figure(figsize=(14,14))\n",
    "    gs = gridspec.GridSpec(2, 4)\n",
    "    gs.update(wspace=0.5)\n",
    "    axes = [plt.subplot(gs[0, :2], ), plt.subplot(gs[0, 2:]), plt.subplot(gs[1, 1:3])]\n",
    "    for each_preds, each_model in preds_zip:\n",
    "        fpr, tpr, thresholds = roc_curve(Y_test, each_preds)\n",
    "        rcpm_change = []\n",
    "        pct_instance = []\n",
    "        for each_thresh in thresholds:\n",
    "            rcpm_change += [expectedValue(each_preds, Y_test, [each_thresh]*len(Y_test), fp_cost, fn_cost)]\n",
    "            pct_instance_thresh = np.sum(each_preds > each_thresh)/len(Y_test)    \n",
    "            pct_instance += [pct_instance_thresh]\n",
    "            \n",
    "        if timeframe=='yearly':\n",
    "            profit = list(map(lambda x: x/1000*12*450*10**9, rcpm_change))\n",
    "        elif timeframe=='monthly':\n",
    "            profit = list(map(lambda x: x/1000*450*10**9, rcpm_change))\n",
    "        else:\n",
    "            raise ValueError('timeframe must be \"yearly\" or \"monthly\"')\n",
    "            \n",
    "        df = pd.DataFrame({'pct_instance': pct_instance, 'profit': profit, 'rcpm_change': rcpm_change})\n",
    "        df = df.sort_values('pct_instance')\n",
    "        df['profit_change'] = df.profit.transform(lambda x: (x/x[0]-1)*-1)\n",
    "        df['profit_diff'] = df.profit.transform(lambda x: (x-x[0]))\n",
    "        \n",
    "        max_idx = df.profit_change.idxmax()\n",
    "        max_profit_change = df.profit_change.iloc[max_idx]\n",
    "        max_profit_pct_instance = df.pct_instance.iloc[max_idx]\n",
    "        max_profit = df.profit.iloc[max_idx]\n",
    "        max_profit_diff = df.profit_diff.iloc[max_idx]\n",
    "\n",
    "        axes[0].plot(df.pct_instance, df.profit_change,\n",
    "                     label = each_model+\" Max: {:0.1%} Pct Inst.: {:0.1%}\".format(\n",
    "                         max_profit_change,max_profit_pct_instance))\n",
    "        axes[0].yaxis.set_major_formatter(pct_formatter)\n",
    "        axes[0].xaxis.set_major_formatter(pct_formatter)\n",
    "\n",
    "        axes[1].plot(df.pct_instance, df.profit_diff,\n",
    "                     label = each_model+\" Max: {}M Pct Inst.: {:0.1%}\".format(\n",
    "                         locale.currency(max_profit_diff*10**-6,grouping=True),max_profit_pct_instance))\n",
    "        axes[1].yaxis.set_major_formatter(mil_formatter)\n",
    "        axes[1].xaxis.set_major_formatter(pct_formatter)\n",
    "        \n",
    "    axes[0].set_title(\"Comparison of Profit Curves (Cost Reduction) on Test Data\")\n",
    "    axes[0].set_xlabel(\"Percentage of Test Instances\")\n",
    "    axes[0].set_ylabel(\"Expected Profit Improvement (Cost Reduction)\")\n",
    "    axes[0].legend()\n",
    "    \n",
    "    axes[1].set_title(\"Comparison of Profit Curves (Cost Reduction) on Test Data\")\n",
    "    axes[1].set_xlabel(\"Percentage of Test Instances\")\n",
    "    axes[1].set_ylabel(\"Expected Profit Improvement (Cost Reduction)\")\n",
    "    axes[1].legend()\n",
    "    \n",
    "    #ROC Plot\n",
    "    for each_preds, each_model in preds_zip:\n",
    "        fpr, tpr, thresholds = roc_curve(Y_test, each_preds)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        axes[2].plot(fpr, tpr, label = each_model+\" (AUC = {:0.3})\".format(roc_auc))\n",
    "\n",
    "    axes[2].set_title(\"Comparison of ROC Curves on Test Data\")\n",
    "    axes[2].set_xlabel(\"fpr\")\n",
    "    axes[2].set_ylabel(\"tpr\")\n",
    "    axes[2].yaxis.set_major_formatter(pct_formatter)\n",
    "    axes[2].xaxis.set_major_formatter(pct_formatter)\n",
    "    axes[2].legend()\n",
    "\n",
    "    return df\n",
    "    \n",
    "def modBootstrapper(train, test, nruns, sampsize, lr, lr_pipeline, svm_pipeline):\n",
    "    X_test = test.drop('label',1)\n",
    "    Y_test = test.label\n",
    "    \n",
    "    aucs = []\n",
    "    for i in range(nruns):\n",
    "        train_sample = train.iloc[np.random.randint(0,train.shape[0], size = sampsize)]\n",
    "        X_train = train_sample.drop('label',1)\n",
    "        Y_train = train_sample.label\n",
    "        \n",
    "        if lr==1:\n",
    "            clf = lr_pipeline\n",
    "            clf.fit(X_train, Y_train)\n",
    "            clf_pos_class = clf.classes_==1\n",
    "            preds = clf.predict_proba(X_test)[:,clf_pos_class]\n",
    "        else:\n",
    "            clf = svm_pipeline\n",
    "            clf.fit(X_train, Y_train)\n",
    "            preds = clf.decision_function(X_test)\n",
    "            \n",
    "        roc_auc = roc_auc_score(Y_test, preds)\n",
    "        aucs += [roc_auc] \n",
    "        \n",
    "    mean_auc = np.mean(aucs)\n",
    "    stderr_auc = np.sqrt(np.var(aucs))\n",
    "    \n",
    "    return mean_auc, stderr_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transformed data\n",
    "cwd = os.getcwd()\n",
    "datadir = cwd + os.sep + 'data' + os.sep\n",
    "\n",
    "def loadSentimentData(fileName):\n",
    "    data = pd.read_csv(datadir + fileName, header=0, index_col=0)\n",
    "    data.dropna(inplace=True)\n",
    "    data.drop(['arousal_mv','valence_mv','label'], 1, inplace=True)\n",
    "    data.index = data.index.astype('int64')\n",
    "    \n",
    "    return data\n",
    "\n",
    "def loadUserData(fileName):\n",
    "    data = pd.read_csv(datadir + fileName, header=0, encoding=\"cp1252\")\n",
    "    data = data[['id','favourites_count','followers_count','friends_count',\n",
    "                 'listed_count','statuses_count', 'label', 'default_profile',\n",
    "                 'default_profile_image','verified','reputation','taste']]\n",
    "    data.set_index('id', inplace=True)\n",
    "    data.dropna(inplace=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def loadTimingData(fileName):\n",
    "    data = pd.read_csv(datadir + fileName, header=0)\n",
    "    data.set_index('user_id', inplace=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def loadData(fileNames):\n",
    "    sentiment = loadSentimentData(fileNames[0])\n",
    "    account = loadUserData(fileNames[1])\n",
    "    timing = loadTimingData(fileNames[2])\n",
    "    data = account.join(sentiment, how='left')\n",
    "    data = data.join(timing, how='left')\n",
    "    \n",
    "    mv_cols = (pd.isnull(data)).any().drop('label')\n",
    "    for each_col, each_bool in zip(mv_cols.index.values, mv_cols):\n",
    "        if each_bool == True:\n",
    "            data[each_col+'_mv'] = np.where(pd.isnull(data[each_col]), 1, 0)\n",
    "            col_mean = data[each_col].mean()\n",
    "            data[each_col] = data[each_col].fillna(col_mean)\n",
    "        \n",
    "    return data\n",
    "    \n",
    "# data = loadData(['sentiment_dist_varol_dump.csv','varol-2017-users.csv','timing.csv'])\n",
    "data = loadData(['sentiment_dist_varol_dump.csv','merged.csv','timing.csv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train test split\n",
    "sample_filt = np.random.uniform(0,1, data.shape[0]) < 0.8\n",
    "\n",
    "X_train = data[sample_filt].drop('label',1)\n",
    "Y_train = data[sample_filt].label\n",
    "X_test = data[~sample_filt].drop('label',1)\n",
    "Y_test = data[~sample_filt].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTree for MI scores\n",
    "dt = DecisionTreeClassifier(criterion='entropy')\n",
    "dt.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get importance and correlation\n",
    "features_summary = pd.DataFrame(list(zip(X_train.columns, dt.feature_importances_)), \n",
    "                                columns=['feature','importance']).set_index('feature')\n",
    "features_summary = features_summary.sort_values('importance', ascending=False)\n",
    "\n",
    "corr_df = pd.DataFrame(data.corr()['label'].drop('label'))\n",
    "corr_df.columns = ['correlation']\n",
    "\n",
    "features_summary = features_summary.merge(corr_df, right_index=True, left_index=True)\n",
    "\n",
    "# Plot importance and correlation\n",
    "color_list = ['r' if corr < 0 else 'g' for corr in features_summary.correlation]\n",
    "features_summary.importance.plot(kind='bar', color=color_list, figsize=(12,8))\n",
    "plt.title('Feature Importance and Correlation Direction')\n",
    "plt.ylabel('Importance')\n",
    "\n",
    "# Select features\n",
    "keep_features = features_summary[features_summary.importance > 0].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Discard features\n",
    "X_train_filt = X_train[keep_features]\n",
    "X_test_filt = X_test[keep_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA & PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(X_test[features_summary.head(10).index], alpha=0.3, figsize=(12, 12), diagonal='kde',\n",
    "              color=np.where(Y_test==1,'C0','C1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "X_test_poly = pd.DataFrame(poly.fit_transform(X_test[features_summary.head(10).index]),\n",
    "                           columns=poly.get_feature_names(),\n",
    "                           index = X_test.index)\n",
    "\n",
    "scatter_matrix(X_test_poly.iloc[:,11:], alpha=0.3, figsize=(20, 20), diagonal='kde',\n",
    "              color=np.where(Y_test==1,'C0','C1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = pd.DataFrame(scale(X_train_filt, axis=0, with_mean=True, with_std=True, copy=True),\n",
    "                            columns = X_train_filt.columns.values,\n",
    "                            index = X_train_filt.index.values)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_train_norm)\n",
    "\n",
    "principle_components_df = pd.DataFrame(pca.transform(X_train_norm), index=X_train_norm.index)\n",
    "principle_components_df = principle_components_df.join(Y_train)\n",
    "fig, axes = plt.subplots(figsize=(8,8))\n",
    "plt.scatter(principle_components_df.iloc[:,0],\n",
    "            principle_components_df.iloc[:,1],\n",
    "            color=['C0' if x==1 else 'C1' for x in principle_components_df['label']],\n",
    "            alpha=0.3)\n",
    "plt.title(\"PCA Decomposition for Test Data\")\n",
    "plt.xlabel(\"PC-1\")\n",
    "plt.ylabel(\"PC-2\")\n",
    "orange_patch = mpatches.Patch(color='C0', label='Non-Human')\n",
    "blue_patch = mpatches.Patch(color='C1', label='Human')\n",
    "plt.legend(handles=[orange_patch, blue_patch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(8,8))\n",
    "axes.plot(pca.explained_variance_ratio_.cumsum())\n",
    "plt.title(\"Cumulative Sum of Explained Variance\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Logistic Regression and SVM\n",
    "lr = LogisticRegression()\n",
    "\n",
    "kfold = KFold(10, True)\n",
    "lr_cv = cross_val_score(lr, X_train, Y_train, cv = kfold, scoring=\"roc_auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LR Mean CV AUC Score: {:0.3}\".format(np.mean(lr_cv))+\n",
    "      \"\\nLR StdErr CV AUC Score: {:0.3}\".format(np.sqrt(np.var(lr_cv)/len(lr_cv))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve for single test split baseline models\n",
    "lr.fit(X_train_filt, Y_train)\n",
    "\n",
    "lr_pos_class = lr.classes_==1\n",
    "preds_lr = lr.predict_proba(X_test_filt)[:,lr_pos_class]\n",
    "preds_zip = zip([preds_lr], [\"LogisticRegression\"])\n",
    "\n",
    "fig, axes = plt.subplots(1,1, figsize=(8,6))\n",
    "for each_preds, each_model in preds_zip:\n",
    "    fpr, tpr, thresholds = roc_curve(Y_test, each_preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    axes.plot(fpr, tpr, label = each_model+\" (AUC = {:0.3})\".format(roc_auc))\n",
    "\n",
    "plt.title(\"ROC Curves for Baseline Model\")\n",
    "plt.xlabel(\"fpr\")\n",
    "plt.ylabel(\"tpr\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipeline = Pipeline([('variance_thresh', VarianceThreshold()),\n",
    "                        ('poly', PolynomialFeatures()),\n",
    "                        ('estimator', LogisticRegression())])\n",
    "\n",
    "tuningIteration(LogisticRegression(),\n",
    "                {'estimator__C': [10**x for x in range(-8,3)],\n",
    "                 'poly__degree': [1,2,3]},\n",
    "                X_train_filt, Y_train,\n",
    "                lr_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pipeline = Pipeline([('variance_thresh', VarianceThreshold()),\n",
    "                        ('normalize', Normalizer()),\n",
    "                        ('estimator', SVC())])\n",
    "\n",
    "tuningIteration(SVC(),\n",
    "                {'estimator__C': [10**x for x in range(-8,3)],\n",
    "                  'estimator__kernel': ['linear','rbf','sigmoid']},\n",
    "                X_train_filt, Y_train,\n",
    "                svm_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR and SVM Learning Curves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipeline = Pipeline([('variance_thresh', VarianceThreshold()),\n",
    "                        ('poly', PolynomialFeatures(2)),\n",
    "                        ('estimator', LogisticRegression(C=10**-3))])\n",
    "\n",
    "svm_pipeline = Pipeline([('variance_thresh', VarianceThreshold()),\n",
    "                        ('normalize', Normalizer()),\n",
    "                        ('estimator', SVC(C=100, kernel='rbf'))])\n",
    "\n",
    "samplesizes = [50, 100, 200, 500, 1000, 1500, 2000]\n",
    "\n",
    "boot_results = defaultdict(list)\n",
    "for i, model in enumerate(['lr','svm']):\n",
    "    for each_samplesize in samplesizes:\n",
    "        mean_auc, stderr_auc = modBootstrapper(X_train_filt.join(Y_train), X_test_filt.join(Y_test),\n",
    "                                               20, each_samplesize, i, lr_pipeline, svm_pipeline)\n",
    "        boot_results[model+\"_mean_auc\"] += [mean_auc]\n",
    "        boot_results[model+\"_stderr_auc\"] += [stderr_auc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,1, figsize=(8,6))\n",
    "x_values = np.log2(boot_results_df.index.values)\n",
    "axes.plot(x_values, boot_results_df.lr_mean_auc, 'r')\n",
    "axes.plot(x_values, boot_results_df.lr_mean_auc - boot_results_df.lr_stderr_auc*2, 'r+')\n",
    "axes.plot(x_values, boot_results_df.lr_mean_auc + boot_results_df.lr_stderr_auc*2, 'r--')\n",
    "axes.plot(x_values, boot_results_df.svm_mean_auc, 'g')\n",
    "axes.plot(x_values, boot_results_df.svm_mean_auc - boot_results_df.svm_stderr_auc*2, 'g+')\n",
    "axes.plot(x_values, boot_results_df.svm_mean_auc + boot_results_df.svm_stderr_auc*2, 'g--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBM Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuningIteration(GradientBoostingClassifier(),\n",
    "                {'estimator__n_estimators': list(range(10,500,20))},\n",
    "                X_train_filt, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuningIteration(GradientBoostingClassifier(n_estimators=70),\n",
    "                {'estimator__max_depth': list(range(1,15))},\n",
    "                X_train_filt, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuningIteration(GradientBoostingClassifier(n_estimators=70, max_depth=2),\n",
    "                {'estimator__min_samples_leaf': list(range(5,500,10))},\n",
    "                X_train_filt, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuningIteration(GradientBoostingClassifier(n_estimators=70, max_depth=2, min_samples_leaf=105),\n",
    "                {'estimator__max_features': list(range(2,X_train_filt.shape[1],2))},\n",
    "                X_train_filt, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuningIteration(GradientBoostingClassifier(n_estimators=70, max_depth=2, min_samples_leaf=105, max_features=8),\n",
    "                {'estimator__subsample': np.array(list(range(10,105,5)))/100},\n",
    "                X_train_filt, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interation 6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuningIteration(GradientBoostingClassifier(n_estimators=70, max_depth=2, min_samples_leaf=105, max_features=8,\n",
    "                                           subsample=0.75),\n",
    "                {'estimator__n_estimators': list(range(10,1000,20)),\n",
    "                 'estimator__learning_rate': [10**x for x in range(-3,0)]},\n",
    "                X_train_filt, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuningIteration(GradientBoostingClassifier(n_estimators=70, max_depth=2, min_samples_leaf=105, max_features=8,\n",
    "                                           subsample=0.75, learning_rate=0.01),\n",
    "                {'estimator__n_estimators': list(range(100,3000,100))},\n",
    "                X_train_filt, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_pipeline = Pipeline([('variance_thresh', VarianceThreshold()),\n",
    "                         ('feat_select', SelectKBest(mutual_info_classif)),\n",
    "                         ('estimator', GradientBoostingClassifier(n_estimators=1700, max_depth=2, min_samples_leaf=105, max_features=8,\n",
    "                                           subsample=0.75, learning_rate=0.01))])\n",
    "\n",
    "max_features = X_train_filt.shape[1] - X_train_filt.shape[1]%2\n",
    "\n",
    "tuningIteration(GradientBoostingClassifier(),\n",
    "                {'feat_select__k': list(range(9,max_features,2))},\n",
    "                X_train_filt, Y_train,\n",
    "                gbm_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_baseline = Pipeline([('estimator', LogisticRegression(random_state=1234))])\n",
    "\n",
    "lr_pipeline_final = Pipeline([('variance_thresh', VarianceThreshold()),\n",
    "                              ('poly', PolynomialFeatures(2)),\n",
    "                              ('estimator', LogisticRegression(C=10**-3, random_state=1234))])\n",
    "                              \n",
    "                              \n",
    "svm_pipeline_final = Pipeline([('variance_thresh', VarianceThreshold()),\n",
    "                               ('normalize', Normalizer()),\n",
    "                               ('estimator', SVC(C=10**2, kernel='rbf', random_state=1234))])\n",
    "\n",
    "                              \n",
    "gbm_pipeline_final = Pipeline([('variance_thresh', VarianceThreshold()),\n",
    "                               ('estimator', GradientBoostingClassifier(n_estimators=1700,max_depth=2,\n",
    "                                                                        min_samples_leaf=105, max_features=8,\n",
    "                                                                        subsample=0.75, learning_rate=0.01,\n",
    "                                                                        random_state=1234))])\n",
    "\n",
    "rf_pipeline_final = Pipeline([('variance_thresh', VarianceThreshold()),\n",
    "                               ('estimator', RandomForestClassifier(n_estimators=421,\n",
    "                                                                    max_features=7,\n",
    "                                                                    min_samples_leaf=8,\n",
    "                                                                    min_samples_split=12,\n",
    "                                                                    random_state=1234))])\n",
    "\n",
    "profit_df = plotPerformance([lr_baseline, lr_pipeline_final, svm_pipeline_final, gbm_pipeline_final, rf_pipeline_final],\n",
    "                [\"Baseline (LR)\",\"LR\", \"SVM\", \"GBM\", \"RF\"],\n",
    "                X_train_filt, Y_train, X_test_filt, Y_test,\n",
    "                -0.03, -0.06, 'yearly')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
